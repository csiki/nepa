# =========
# This file is generated by LLM
# =========

import argparse
import io
import os
import random

import matplotlib.pyplot as plt
import numpy as np
import requests
import torch
import torch.nn.functional as F
from PIL import Image, ImageDraw
from datasets import load_dataset
from transformers import AutoImageProcessor

from models.vit_nepa import ViTNepaForImageClassification, ViTNepaModel


# =========
# Utilities
# =========

def normalize_except_zero(arr, tol=None, temperature=1.0):
    """
    Keep zeros (and near-zeros) at 0; linearly normalize non-zero values to [0, 1].
    tol: absolute tolerance for "near-zero"; if None, inferred from data scale.
    temperature: exponent applied after normalization to adjust contrast.
    """
    arr = np.asarray(arr)
    out = np.zeros_like(arr, dtype=float)

    if tol is None:
        max_abs = np.max(np.abs(arr)) if arr.size else 1.0
        tol = 1e3 * np.finfo(float).eps * max(1.0, max_abs)

    mask = ~np.isclose(arr, 0.0, atol=tol, rtol=0.0)

    if not np.any(mask):
        return out

    m = arr[mask].min()
    M = arr[mask].max()
    if M > m:
        s01 = (arr[mask] - m) / (M - m)
        sT = s01 ** (1.0 / temperature)
        out[mask] = m + (1.0 - m) * sT
    else:
        out[mask] = 1.0

    return out


def load_image(img_path: str, img_url: str) -> Image.Image:
    """Load image from local path if it exists; otherwise download from URL."""
    if img_path and os.path.exists(img_path):
        img = Image.open(img_path).convert("RGB")
        return img
    r = requests.get(img_url, timeout=10)
    r.raise_for_status()
    img = Image.open(io.BytesIO(r.content)).convert("RGB")
    return img


def get_hw_from_processor(image_processor):
    """Infer processed image height and width from the image processor."""
    size = getattr(image_processor, "size", None)
    if isinstance(size, dict):
        h = size.get("height", size.get("shortest_edge", 224))
        w = size.get("width", size.get("shortest_edge", 224))
    elif isinstance(size, int):
        h = w = size
    else:
        h = w = 224
    return int(h), int(w)


def get_patch_grid(config, proc_h, proc_w):
    """Return grid size (H, W) and patch size (ph, pw) given model config."""
    patch = getattr(config, "patch_size", 16)
    if isinstance(patch, (list, tuple)):
        ph, pw = patch
    else:
        ph = pw = int(patch)
    gh, gw = proc_h // ph, proc_w // pw
    return gh, gw, ph, pw


def draw_patch_box(
    img: Image.Image,
    grid_h,
    grid_w,
    ph,
    pw,
    patch_index,
    color=(255, 0, 0),
    width=3,
):
    """Draw a rectangle around a specific patch index on the grid."""
    r = patch_index // grid_w
    c = patch_index % grid_w
    x0 = c * pw
    y0 = r * ph
    x1 = x0 + pw
    y1 = y0 + ph
    img_boxed = img.copy()
    draw = ImageDraw.Draw(img_boxed)
    for i in range(width):
        draw.rectangle([x0 - i, y0 - i, x1 + i, y1 + i], outline=color)
    return img_boxed, (r, c, x0, y0, x1, y1)


def attentions_from_outputs(outputs, layer_idx):
    """
    Flexible attention selection:
      - int: single layer
      - list/tuple: average over selected layers
      - 'all' or None: average over all layers
    Returns: [B, H, T, T]
    """
    atts = outputs.attentions
    if atts is None:
        raise RuntimeError(
            "Model did not return attentions. "
            "Ensure forward is called with output_attentions=True "
            "and the model supports it."
        )
    L = len(atts)

    def norm_idx(i):
        return i if i >= 0 else L + i

    if layer_idx is None or layer_idx == "all":
        idxs = list(range(L))
    elif isinstance(layer_idx, (list, tuple)):
        idxs = [norm_idx(int(i)) for i in layer_idx]
    else:
        i = norm_idx(int(layer_idx))
        return atts[i]

    sel = [atts[i] for i in idxs]
    att_mean = torch.stack(sel, dim=0).mean(0)  # [B,H,T,T]
    return att_mean


def aggregate_heads(att, mode="mean"):
    """Aggregate attention heads: mean, max, or none."""
    if mode == "mean":
        return att.mean(dim=1, keepdim=True)  # [B,1,T,T]
    elif mode == "max":
        return att.max(dim=1, keepdim=True).values
    elif mode == "none":
        return att
    else:
        raise ValueError(f"Unknown HEAD_REDUCTION: {mode}")


def attention_map_for_patch(att_layer, token_index):
    """Return attention vector over tokens for a given query token index."""
    return att_layer[:, :, token_index, :]  # [B, H', T]


def tokens_to_grid(vec, grid_h, grid_w):
    """
    Remove CLS/BOS (index=0) and reshape to [grid_h, grid_w].
    Supported shapes:
      - [B,H',T]
      - [B,T]
      - [T]
    """
    if vec.dim() == 3:
        vec_ = vec[..., 1:]
        if vec_.dim() == 3:
            vec_ = vec_[0, 0]
    elif vec.dim() == 2:
        vec_ = vec[:, 1:][0]
    elif vec.dim() == 1:
        vec_ = vec[1:]
    else:
        raise ValueError("Unsupported vec shape for tokens_to_grid.")
    return vec_.reshape(grid_h, grid_w)


def to_display_image(pixel_values, image_processor):
    """Convert model input tensor back to a displayable PIL image."""
    arr = pixel_values[0].detach().cpu().numpy()
    mean = np.array(getattr(image_processor, "image_mean", [0.5, 0.5, 0.5])).reshape(
        3, 1, 1
    )
    std = np.array(getattr(image_processor, "image_std", [0.5, 0.5, 0.5])).reshape(
        3, 1, 1
    )
    arr = arr * std + mean
    arr = np.clip(arr, 0.0, 1.0)
    arr = np.transpose(arr, (1, 2, 0))  # HWC
    arr = (arr * 255).astype(np.uint8)
    return Image.fromarray(arr)


def render_grid_figure(
    proc_img,
    grid_2d,
    title="",
    overlay=True,
    alpha=0.55,
    cmap="jet",
    gamma=0.5,
    temperature=1.0,
    save_path=None,
):
    """
    Generic heatmap-on-grid visualization:
    - Optionally overlay on image
    - Apply gamma for contrast
    - Optionally save to disk
    """
    import matplotlib.patches as patches

    H_img, W_img = proc_img.size[1], proc_img.size[0]  # PIL: (w, h)
    arr = np.array(grid_2d.cpu(), dtype=np.float32)

    arr_norm = normalize_except_zero(arr, temperature=temperature)
    arr_vis = np.power(arr_norm, gamma)

    gh, gw = arr_vis.shape
    cell_w = W_img / gw
    cell_h = H_img / gh

    plt.figure(figsize=(5, 5))
    ax = plt.gca()

    if overlay:
        ax.imshow(proc_img)
    else:
        ax.set_facecolor("white")
        ax.set_xlim([0, W_img])
        ax.set_ylim([H_img, 0])

    cmap_obj = plt.get_cmap(cmap)

    for r in range(gh):
        for c in range(gw):
            v = float(arr_vis[r, c])
            rgba = list(cmap_obj(v))
            rgba[3] = alpha

            x0, y0 = c * cell_w, r * cell_h
            rect = patches.Rectangle(
                (x0, y0),
                cell_w,
                cell_h,
                linewidth=0,
                edgecolor="none",
                facecolor=rgba,
            )
            ax.add_patch(rect)

    ax.set_xlim([0, W_img])
    ax.set_ylim([H_img, 0])
    ax.set_xticks([])
    ax.set_yticks([])
    plt.axis("off")

    if title:
        ax.set_title(title)

    if save_path is not None:
        plt.savefig(save_path, bbox_inches="tight", pad_inches=0)
        print(f"[Saved] {save_path}")


def hidden_input_prob_grid(
    outputs,
    token_index,
    grid_h,
    grid_w,
    top_p=None,
    renormalize_topn=False,
):
    """
    Compute probability distribution of predicted hidden vs all input embeddings
    for a selected token, using cosine similarity and optional top-p (nucleus)
    filtering over spatial tokens (excluding CLS/BOS).
    Returns the first batch as a [grid_h, grid_w] tensor.
    """
    if hasattr(outputs, "last_hidden_state"):
        h_pred = outputs.last_hidden_state  # [B, T, D]
    elif isinstance(outputs, dict) and "last_hidden_state" in outputs:
        h_pred = outputs["last_hidden_state"]
    else:
        raise RuntimeError("outputs does not contain last_hidden_state")

    if hasattr(outputs, "input_embedding"):
        e_in = outputs.input_embedding  # [B, T, D]
    elif isinstance(outputs, dict) and "input_embedding" in outputs:
        e_in = outputs["input_embedding"]
    else:
        raise RuntimeError("outputs does not contain input_embedding")

    B, T, D = h_pred.shape
    if token_index < 1 or token_index >= T:
        raise ValueError(f"token_index={token_index} out of range, expected [1, {T - 1}]")

    h = h_pred[:, token_index, :]  # [B, D]

    e = F.normalize(e_in[:, 1:], dim=-1)  # [B, T-1, D]
    h = F.normalize(h.unsqueeze(1), dim=-1)  # [B, 1, D]
    sim = (e * h).sum(dim=-1)  # [B, T-1]

    prob_no_cls = torch.softmax(sim, dim=-1)  # [B, T-1]

    if top_p is not None:
        p = float(top_p)
        if not (0.0 < p <= 1.0):
            raise ValueError(f"top_p={top_p} must be in (0, 1].")

        sorted_probs, sorted_idx = torch.sort(
            prob_no_cls, dim=-1, descending=True
        )  # [B, T-1]
        cumsum = sorted_probs.cumsum(dim=-1)

        keep_mask_sorted = (cumsum - sorted_probs) < p  # [B, T-1] bool

        keep_mask = torch.zeros_like(prob_no_cls, dtype=torch.bool)
        keep_mask.scatter_(dim=-1, index=sorted_idx, src=keep_mask_sorted)

        prob_no_cls = prob_no_cls * keep_mask.to(prob_no_cls.dtype)

        if renormalize_topn:
            denom = prob_no_cls.sum(dim=-1, keepdim=True).clamp_min(1e-12)
            prob_no_cls = prob_no_cls / denom

    grid = prob_no_cls[0].reshape(grid_h, grid_w)
    return grid


def save_inference(
    idx,
    proc_img_pil,
    boxed_img,
    att_grid,
    prob_grid,
    overlay,
    alpha,
    gamma_att,
    gamma_prob,
    save_dir,
    class_id,
    random_seed,
):
    """Save visualization artifacts for one sample (no on-screen display)."""
    dir_name = f"{save_dir}/{class_id}_{random_seed}"
    os.makedirs(dir_name, exist_ok=True)
    prefix = f"{dir_name}/img_{idx}"

    # save preprocessed image
    proc_img_pil.save(f"{prefix}_proc.png")

    # save patch-highlighted image
    boxed_img.save(f"{prefix}_patch.png")

    # save attention heatmap
    render_grid_figure(
        proc_img_pil,
        att_grid,
        overlay=overlay,
        alpha=alpha,
        gamma=gamma_att,
        temperature=1.0,
        save_path=f"{prefix}_att.png",
    )

    # save probability heatmap
    render_grid_figure(
        proc_img_pil,
        prob_grid,
        overlay=overlay,
        alpha=alpha,
        gamma=gamma_prob,
        temperature=1.0,
        save_path=f"{prefix}_prob.png",
    )


def get_random_batch(dataset, batch_size):
    """Sample a random batch from a streaming dataset."""
    return list(dataset.shuffle(seed=random.randint(0, 999999)).take(batch_size))


def get_batch_by_class_id(dataset, batch_size, class_id, dataset_name):
    """
    In streaming ImageNet, collect `batch_size` samples with the given class_id.
    Currently supports imagenet-1k only.
    """
    batch = []
    it = dataset.shuffle(seed=random.randint(0, 999999))

    for example in it:
        if dataset_name == "imagenet-1k":
            if example["label"] == class_id:
                batch.append(example)
        else:
            raise ValueError(
                "get_batch_by_class_id currently supports 'imagenet-1k' only"
            )

        if len(batch) == batch_size:
            break

    if len(batch) < batch_size:
        raise ValueError(
            f"Not enough samples with class_id={class_id}. "
            f"Requested {batch_size}, got {len(batch)}."
        )
    return batch


def extract_image(example, dataset_name):
    """Extract PIL image from dataset example."""
    if dataset_name in ("imagenet-1k", "coco_captions"):
        return example["image"]
    raise ValueError(f"Unsupported dataset: {dataset_name}")


# =========
# CLI parsing
# =========

def parse_layer_index(arg: str):
    """
    Parse layer index argument:
      - "all" or empty -> None (means all layers)
      - "0,1,2" -> [0,1,2]
      - single int string -> int
    """
    if arg is None or arg.strip() == "" or arg.strip().lower() == "all":
        return None  # will be treated as "all" inside attentions_from_outputs
    parts = [p.strip() for p in arg.split(",") if p.strip() != ""]
    if len(parts) == 1:
        return int(parts[0])
    return [int(p) for p in parts]


def build_arg_parser():
    parser = argparse.ArgumentParser(
        description="ViT attention & embedding analysis for ViTNepa models."
    )

    # Model & HF
    parser.add_argument(
        "--model-id",
        type=str,
        default=None,
        help="Hugging Face model id.",
    )
    parser.add_argument(
        "--revision",
        type=str,
        default=None,
        help="Model revision / checkpoint on Hugging Face.",
    )
    parser.add_argument(
        "--token",
        type=str,
        default=None,
        help="HF token for private model/dataset (optional).",
    )
    parser.add_argument(
        "--use-classification-head",
        action="store_true",
        help="Use ViTNepaForImageClassification instead of ViTNepaModel.",
    )

    # Dataset
    parser.add_argument(
        "--dataset-name",
        type=str,
        default="imagenet-1k",
        choices=["imagenet-1k", "coco_captions"],
        help="Dataset name.",
    )
    parser.add_argument(
        "--split",
        type=str,
        default="validation",
        help="Dataset split (for streaming).",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=8,
        help="Number of images to sample for analysis.",
    )
    parser.add_argument(
        "--class-id",
        type=int,
        default=388,
        help="Target class id for ImageNet sampling.",
    )

    # Randomness / device
    parser.add_argument(
        "--seed",
        type=int,
        default=6,
        help="Random seed for sampling and patch selection.",
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help='Device to run on ("cuda", "cpu"); default: auto.',
    )

    # Attention & viz
    parser.add_argument(
        "--layer-index",
        type=str,
        default="0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15",
        help='Layers to use for attention. '
             'Examples: "all", "0", "0,1,2".',
    )
    parser.add_argument(
        "--head-reduction",
        type=str,
        default="mean",
        choices=["mean", "max", "none"],
        help="How to aggregate attention heads.",
    )
    parser.add_argument(
        "--no-overlay",
        action="store_true",
        help="Do not overlay heatmaps on the image (heatmap only).",
    )
    parser.add_argument(
        "--alpha-overlay",
        type=float,
        default=0.55,
        help="Alpha for heatmap overlay.",
    )
    parser.add_argument(
        "--gamma",
        type=float,
        default=0.25,
        help="Gamma for attention heatmap contrast.",
    )

    # Embedding prob settings
    parser.add_argument(
        "--top-p",
        type=float,
        default=1.0,
        help="Top-p (nucleus) value for probability grid (0 < p <= 1).",
    )
    parser.add_argument(
        "--no-renorm",
        action="store_true",
        help="Do not renormalize probabilities after top-p filtering.",
    )
    parser.add_argument(
        "--emb-gamma",
        type=float,
        default=10.0,
        help="Gamma for embedding probability heatmap contrast.",
    )

    # Saving
    parser.add_argument(
        "--save-dir",
        type=str,
        default="viz_output",
        help="Base directory to save visualizations.",
    )

    return parser


def main():
    parser = build_arg_parser()
    args = parser.parse_args()

    # Config from CLI
    MODEL_ID = args.model_id
    REVISION = args.revision
    TOKEN = args.token

    USE_CLASSIFICATION_HEAD = args.use_classification_head
    DATASET_NAME = args.dataset_name
    SPLIT = args.split
    BATCH_SIZE = args.batch_size
    CLASS_ID = args.class_id
    RANDOM_SEED = args.seed

    LAYER_INDEX = parse_layer_index(args.layer_index)

    HEAD_REDUCTION = args.head_reduction
    SHOW_OVERLAY = not args.no_overlay
    ALPHA_OVERLAY = args.alpha_overlay
    GAMMA = args.gamma
    TOP_P = args.top_p
    RENORMALIZE_TOPN = not args.no_renorm
    EMB_GAMMA = args.emb_gamma

    SAVE_DIR = args.save_dir

    torch.set_grad_enabled(False)
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)

    if args.device is not None:
        device = args.device
    else:
        device = "cuda" if torch.cuda.is_available() else "cpu"

    # =========
    # Load model
    # =========
    image_processor = AutoImageProcessor.from_pretrained(
        MODEL_ID, revision=REVISION, token=TOKEN
    )

    if USE_CLASSIFICATION_HEAD:
        model = ViTNepaForImageClassification.from_pretrained(
            MODEL_ID, revision=REVISION, trust_remote_code=True, token=TOKEN
        )
    else:
        model = ViTNepaModel.from_pretrained(
            MODEL_ID, revision=REVISION, trust_remote_code=True, token=TOKEN
        )
    model.to(device).eval()

    # =========
    # Load dataset (streaming)
    # =========
    dataset = load_dataset(DATASET_NAME, split=SPLIT, streaming=True, token=TOKEN)

    if DATASET_NAME == "imagenet-1k":
        batch = get_batch_by_class_id(dataset, BATCH_SIZE, CLASS_ID, DATASET_NAME)
    else:
        # For other datasets just take random batch
        batch = get_random_batch(dataset, BATCH_SIZE)

    imgs = [extract_image(x, DATASET_NAME) for x in batch]

    proc_h, proc_w = get_hw_from_processor(image_processor)
    grid_h, grid_w, ph, pw = get_patch_grid(model.config, proc_h, proc_w)

    for i in range(BATCH_SIZE):
        img_pil = imgs[i]

        single_inputs = image_processor(img_pil, return_tensors="pt")
        single_inputs = {k: v.to(device) for k, v in single_inputs.items()}

        outputs = model(**single_inputs, output_attentions=True)

        proc_img_pil = to_display_image(single_inputs["pixel_values"], image_processor)

        att = attentions_from_outputs(outputs, LAYER_INDEX)
        att = aggregate_heads(att, HEAD_REDUCTION)  # [1, H', T, T]

        H = random.randint(0, grid_h - 1)
        W = random.randint(0, grid_w - 1)
        token_index = 1 + H * grid_w + W  # skip CLS

        boxed_img, _ = draw_patch_box(
            proc_img_pil,
            grid_h,
            grid_w,
            ph,
            pw,
            patch_index=(token_index - 1),
        )

        att_vec = attention_map_for_patch(att, token_index)
        att_grid = tokens_to_grid(att_vec, grid_h, grid_w)

        prob_grid = hidden_input_prob_grid(
            outputs,
            token_index,
            grid_h,
            grid_w,
            top_p=TOP_P,
            renormalize_topn=RENORMALIZE_TOPN,
        )

        print(f"\n====== Sample {i} | patch=({H},{W}) token={token_index} ======")
        save_inference(
            i,
            proc_img_pil,
            boxed_img,
            att_grid,
            prob_grid,
            overlay=SHOW_OVERLAY,
            alpha=ALPHA_OVERLAY,
            gamma_att=GAMMA,
            gamma_prob=EMB_GAMMA,
            save_dir=SAVE_DIR,
            class_id=CLASS_ID,
            random_seed=RANDOM_SEED,
        )


if __name__ == "__main__":
    main()
